{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用llamaindex构建rag引擎时，有一下方式：\n",
    "\n",
    "- 标准的RAG过程，所有文档构建1个增强查询引擎\n",
    "- 单独为每个文档构建1个查询引擎，然后让llm选择查询引擎使用\n",
    "\n",
    "本脚本比较他们之间的差异，评估不同方法的效果\n",
    "\n",
    "|方法|answer_relevancy|context_relevancy|correctness|faithfulness|\n",
    "|---|---|---|---|---|\n",
    "|所有文档1个引擎|0.85|0.84375|2.95|0.25|\n",
    "|每个文档1个引擎|0.7|0.70625|3.025|0.25|\n",
    "\n",
    "指标只是有相对参考性，原因如下：1）没有使用私域数据，文档内的知识可能llm本身就具备；2) 没有定制prompt，不同方式的倾向不同，有的方法擅长给出步骤，有的方法擅长总结\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "base_url='http://localhost:11434'\n",
    "llm = Ollama(model=\"qwen2.5:latest\", request_timeout=360.0,base_url=base_url)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"quentinz/bge-large-zh-v1.5:latest\",base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable async for the notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"500px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.llama_dataset import RagPredictionDataset\n",
    "\n",
    "async def Build_test_dataset(nodes,query_engine,test_size=10,data_dir='./data',prefix=''):\n",
    "    ragdataset_path=os.path.join(data_dir,f'ragdataset.json')\n",
    "    ragdataset_predictions_path=os.path.join(data_dir,f'{prefix}-ragdataset_predictions.json')\n",
    "\n",
    "    if os.path.exists(ragdataset_path):\n",
    "        rag_dataset=LabeledRagDataset.from_json(ragdataset_path)\n",
    "    else:\n",
    "        DEFAULT_QUESTION_GENERATION_PROMPT = \"\"\"\\\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        {context_str}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge.\n",
    "        generate only questions based on the below query.\n",
    "        使用中文生成答案\n",
    "        {query_str}\n",
    "        \"\"\"\n",
    "\n",
    "        DEFAULT_TEXT_QA_PROMPT_TMPL=(\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge,answer the query.\\n\"\n",
    "            \"使用中文生成答案\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "        text_qa_template = PromptTemplate(\n",
    "            DEFAULT_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n",
    "        )\n",
    "\n",
    "        text_question_template=PromptTemplate(DEFAULT_QUESTION_GENERATION_PROMPT)\n",
    "\n",
    "        num_questions_per_chunk=1\n",
    "        # role=\"Teacher/Professor\"\n",
    "        role=\"小说作家\"\n",
    "        question_gen_query=f\"\"\"\n",
    "            You are a {role}. \n",
    "            Your task is to setup {num_questions_per_chunk} questions for an upcoming quiz/examination. \n",
    "            The questions should be diverse in nature across the document. \n",
    "            Restrict the questions to the context information provided. \n",
    "        \"\"\"\n",
    "\n",
    "        import random\n",
    "        random.seed(0)\n",
    "        test_size=min(len(nodes),test_size)\n",
    "        sample_nodes=random.sample(nodes,test_size)\n",
    "\n",
    "        # step1:初始化数据生成器\n",
    "        print('step1:初始化数据生成器')\n",
    "        rag_dataset_generator=RagDatasetGenerator(nodes=sample_nodes,\n",
    "                                                text_question_template=text_question_template,\n",
    "                                                text_qa_template=text_qa_template,\n",
    "                                                question_gen_query=question_gen_query,\n",
    "                                                num_questions_per_chunk=num_questions_per_chunk)\n",
    "\n",
    "        # step2:为每个node生成问题（包含标准答案）\n",
    "        print('step2:为每个node生成问题（包含标准答案）')\n",
    "        rag_dataset = rag_dataset_generator.generate_dataset_from_nodes()\n",
    "        rag_dataset.save_json(ragdataset_path)\n",
    "\n",
    "    if os.path.exists(ragdataset_predictions_path):\n",
    "        rag_predictions_dataset=RagPredictionDataset.from_json(ragdataset_predictions_path)\n",
    "    else:\n",
    "        # step3:使用query_engine回答问题\n",
    "        print('step3:使用query_engine回答问题')\n",
    "        rag_predictions_dataset=await rag_dataset.amake_predictions_with(\n",
    "            predictor=query_engine,\n",
    "            batch_size=10,\n",
    "            sleep_time_in_seconds=2)\n",
    "        rag_predictions_dataset.save_json(ragdataset_predictions_path)\n",
    "    \n",
    "    return rag_dataset,rag_predictions_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    ")\n",
    "\n",
    "runner=BatchEvalRunner(\n",
    "    evaluators={\n",
    "        \"answer_relevancy\":AnswerRelevancyEvaluator(),\n",
    "        \"context_relevancy\":ContextRelevancyEvaluator(),\n",
    "        \"correctness\":CorrectnessEvaluator(),\n",
    "        \"faithfulness\":FaithfulnessEvaluator()        \n",
    "    },\n",
    "    workers=12,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "async def eval_query_engine(queries:List[str],contexts_list:List[List[str]],response_strs:List[str]):\n",
    "    eval_results=await runner.aevaluate_response_strs(\n",
    "        queries=queries,\n",
    "        contexts_list=contexts_list,\n",
    "        response_strs=response_strs\n",
    "    )\n",
    "\n",
    "    for key in eval_results.keys():\n",
    "        results = eval_results[key]\n",
    "        scores = 0\n",
    "        for result in results:\n",
    "            score = getattr(result,'score',0)\n",
    "            if score:\n",
    "                scores += score\n",
    "        score = scores / len(results)\n",
    "        print(f\"{key} Score: {score}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 所有文档构建1个查询引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files:   0%|          | 0/140 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 140/140 [00:00<00:00, 521.89file/s]\n",
      "c:\\Users\\wushaogui\\miniconda3\\envs\\langchian\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 140/140 [00:01<00:00, 124.27it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "documents=SimpleDirectoryReader(input_dir='../../data/sidaminzhu',recursive=True).load_data(show_progress=True)\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents,show_progress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating embeddings: 100%|██████████| 962/962 [01:08<00:00, 14.14it/s]\n"
     ]
    }
   ],
   "source": [
    "# 生成索引及查询引擎\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index=VectorStoreIndex(nodes=nodes,show_progress=True)\n",
    "query_engine=index.as_query_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step1:初始化数据生成器\n",
      "step2:为每个node生成问题（包含标准答案）\n",
      "step3:使用query_engine回答问题\n"
     ]
    }
   ],
   "source": [
    "rag_dataset,rag_predictions_dataset=await Build_test_dataset(\n",
    "    nodes,query_engine,test_size=20,prefix='OneEngine')\n",
    "\n",
    "# 查看测试数据\n",
    "# import pandas as pd\n",
    "# show_num=1\n",
    "# contexts_query_answer={\n",
    "#     \"上下文\":[example.reference_contexts for example in rag_dataset.examples[:show_num]],\n",
    "#     \"生成的提问\":[example.query for example in rag_dataset.examples[:show_num]],\n",
    "#     \"回答\":[example.response for example in rag_predictions_dataset.predictions[:show_num]],\n",
    "# }\n",
    "# df=pd.DataFrame(contexts_query_answer)\n",
    "# displayify_df(df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [05:51<00:00,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy Score: 0.85\n",
      "context_relevancy Score: 0.84375\n",
      "correctness Score: 2.95\n",
      "faithfulness Score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queries=[example.query for example in rag_dataset.examples]\n",
    "contexts_list=[example.reference_contexts for example in rag_dataset.examples]\n",
    "response_strs=[example.response for example in rag_predictions_dataset.predictions]\n",
    "\n",
    "# 已有回答，直接传入回答评估\n",
    "await eval_query_engine(queries,contexts_list,response_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每个文档构建1个查询引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 41/41 [00:00<00:00, 2022.33file/s]\n",
      "Loading files: 100%|██████████| 34/34 [00:00<00:00, 2748.14file/s]\n",
      "Loading files: 100%|██████████| 26/26 [00:00<00:00, 905.32file/s]\n",
      "Loading files: 100%|██████████| 39/39 [00:00<00:00, 2479.24file/s]\n",
      "Generating embeddings: 100%|██████████| 127/127 [00:09<00:00, 13.40it/s]\n",
      "Generating embeddings: 100%|██████████| 311/311 [00:21<00:00, 14.63it/s]\n",
      "Generating embeddings: 100%|██████████| 340/340 [00:23<00:00, 14.74it/s]\n",
      "Generating embeddings: 100%|██████████| 184/184 [00:12<00:00, 15.11it/s]\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "dir_list=glob.glob('../../data/sidaminzhu/*')\n",
    "documents=[\n",
    "    SimpleDirectoryReader(input_dir).load_data(show_progress=True)\n",
    "    for input_dir in dir_list\n",
    " ]\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "documents_nodes=[\n",
    "    splitter.get_nodes_from_documents(document)\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "documents_indexs=[\n",
    "    VectorStoreIndex(nodes=nodes,show_progress=True)\n",
    "    for nodes in documents_nodes\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_doc_tools(\n",
    "    vector_index,name: str,\n",
    ") -> str:\n",
    "    \n",
    "    def vector_query(query: str) -> str:\n",
    "        f'''设计用于回答关于{name}的问题\n",
    "        query : 输入内容\n",
    "        '''\n",
    "\n",
    "        query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\", fn=vector_query,description=f\"关于{name}问题的回答助手\"\n",
    "    )\n",
    "\n",
    "    return vector_query_tool\n",
    "\n",
    "dir_info=[os.path.split(dir)[1].replace('白话文','') for dir in dir_list]\n",
    "documents_tools=[\n",
    "    get_doc_tools(vector_index,dir_info[i])\n",
    "    for i,vector_index in enumerate(documents_indexs)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.objects import ObjectIndex\n",
    "\n",
    "tool_index=ObjectIndex.from_objects(\n",
    "    documents_tools,\n",
    "    index_cls=VectorStoreIndex\n",
    ")\n",
    "\n",
    "tool_retriever=tool_index.as_retriever(similarity_top_k=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tool_retriever=tool_retriever,\n",
    "    system_prompt=\"\"\"You are an agent designed to answer queries over a set of given documents.\n",
    "    Please use the tools provided to answer a question as possible. Do not rely on prior knowledge\\\n",
    "    \"\"\",\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step3:使用query_engine回答问题\n"
     ]
    }
   ],
   "source": [
    "rag_dataset,rag_predictions_dataset=await Build_test_dataset(\n",
    "    nodes,agent,test_size=20,prefix='MulEngine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [05:23<00:00,  4.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy Score: 0.7\n",
      "context_relevancy Score: 0.70625\n",
      "correctness Score: 3.025\n",
      "faithfulness Score: 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "queries=[example.query for example in rag_dataset.examples]\n",
    "contexts_list=[example.reference_contexts for example in rag_dataset.examples]\n",
    "response_strs=[example.response for example in rag_predictions_dataset.predictions]\n",
    "\n",
    "# 已有回答，直接传入回答评估\n",
    "await eval_query_engine(queries,contexts_list,response_strs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
