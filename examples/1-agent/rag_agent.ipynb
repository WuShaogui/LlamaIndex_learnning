{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在使用 llamaindex 构建 rag 引擎时，可以有以下方式：\n",
    "- 所有文档 1 个引擎\n",
    "- 每个文档1 个引擎：对每个文档构建引擎\n",
    "- 每个文档 2 个引擎：对每个文档，构建2种查询引擎，比如关键字、向量等，然后通过llm选择查询引擎\n",
    "\n",
    "|方法|answer_relevancy|context_relevancy|correctness|faithfulness|\n",
    "|---|---|---|---|---|\n",
    "|所有文档1个引擎|0.75|0.7375|2.925|0.15|\n",
    "|每个文档1个引擎|0.7|0.866875|2.825|0.15|\n",
    "|每个文档2个引擎|0.775|0.76375|2.9|0.15|\n",
    "|每个文档2个引擎2|0.775|0.745|2.925|0.2|\n",
    "\n",
    "指标只是有相对参考性，原因如下：1）没有使用私域数据，文档内的知识可能llm本身就具备；2) 没有定制prompt，不同方式的倾向不同，有的方法擅长给出步骤，有的方法擅长总结;3) 测试数据有限，数据太少导致指标出现偏差\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import Settings\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "\n",
    "base_url='http://localhost:11434'\n",
    "llm = Ollama(model=\"qwen2.5:latest\", request_timeout=360.0,base_url=base_url)\n",
    "Settings.llm = llm\n",
    "Settings.embed_model = OllamaEmbedding(model_name=\"quentinz/bge-large-zh-v1.5:latest\",base_url=base_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 生成测试数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable async for the notebook\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayify_df(df):\n",
    "    \"\"\"For pretty displaying DataFrame in a notebook.\"\"\"\n",
    "    display_df = df.style.set_properties(\n",
    "        **{\n",
    "            \"inline-size\": \"500px\",\n",
    "            \"overflow-wrap\": \"break-word\",\n",
    "        }\n",
    "    )\n",
    "    display(display_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from llama_index.core.llama_dataset.generator import RagDatasetGenerator\n",
    "from llama_index.core.prompts.base import PromptTemplate\n",
    "from llama_index.core.prompts.prompt_type import PromptType\n",
    "from llama_index.core.llama_dataset import LabeledRagDataset\n",
    "from llama_index.core.llama_dataset import RagPredictionDataset\n",
    "\n",
    "async def Build_test_dataset(nodes,query_engine,test_size=10,data_dir='./data',prefix=''):\n",
    "    ragdataset_path=os.path.join(data_dir,f'ragdataset.json')\n",
    "    ragdataset_predictions_path=os.path.join(data_dir,f'{prefix}-ragdataset_predictions.json')\n",
    "\n",
    "    if os.path.exists(ragdataset_path):\n",
    "        rag_dataset=LabeledRagDataset.from_json(ragdataset_path)\n",
    "    else:\n",
    "        DEFAULT_QUESTION_GENERATION_PROMPT = \"\"\"\\\n",
    "        Context information is below.\n",
    "        ---------------------\n",
    "        {context_str}\n",
    "        ---------------------\n",
    "        Given the context information and not prior knowledge.\n",
    "        generate only questions based on the below query.\n",
    "        使用中文生成答案\n",
    "        {query_str}\n",
    "        \"\"\"\n",
    "\n",
    "        DEFAULT_TEXT_QA_PROMPT_TMPL=(\n",
    "            \"Context information is below.\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"{context_str}\\n\"\n",
    "            \"---------------------\\n\"\n",
    "            \"Given the context information and not prior knowledge,answer the query.\\n\"\n",
    "            \"使用中文生成答案\\n\"\n",
    "            \"Query: {query_str}\\n\"\n",
    "            \"Answer: \"\n",
    "        )\n",
    "\n",
    "        text_qa_template = PromptTemplate(\n",
    "            DEFAULT_TEXT_QA_PROMPT_TMPL, prompt_type=PromptType.QUESTION_ANSWER\n",
    "        )\n",
    "\n",
    "        text_question_template=PromptTemplate(DEFAULT_QUESTION_GENERATION_PROMPT)\n",
    "\n",
    "        num_questions_per_chunk=1\n",
    "        # role=\"Teacher/Professor\"\n",
    "        role=\"小说作家\"\n",
    "        question_gen_query=f\"\"\"\n",
    "            You are a {role}. \n",
    "            Your task is to setup {num_questions_per_chunk} questions for an upcoming quiz/examination. \n",
    "            The questions should be diverse in nature across the document. \n",
    "            Restrict the questions to the context information provided. \n",
    "        \"\"\"\n",
    "\n",
    "        import random\n",
    "        random.seed(0)\n",
    "        test_size=min(len(nodes),test_size)\n",
    "        sample_nodes=random.sample(nodes,test_size)\n",
    "\n",
    "        # step1:初始化数据生成器\n",
    "        print('step1:初始化数据生成器')\n",
    "        rag_dataset_generator=RagDatasetGenerator(nodes=sample_nodes,\n",
    "                                                text_question_template=text_question_template,\n",
    "                                                text_qa_template=text_qa_template,\n",
    "                                                question_gen_query=question_gen_query,\n",
    "                                                num_questions_per_chunk=num_questions_per_chunk)\n",
    "\n",
    "        # step2:为每个node生成问题（包含标准答案）\n",
    "        print('step2:为每个node生成问题（包含标准答案）')\n",
    "        rag_dataset = rag_dataset_generator.generate_dataset_from_nodes()\n",
    "        rag_dataset.save_json(ragdataset_path)\n",
    "\n",
    "    if os.path.exists(ragdataset_predictions_path):\n",
    "        rag_predictions_dataset=RagPredictionDataset.from_json(ragdataset_predictions_path)\n",
    "    else:\n",
    "        # step3:使用query_engine回答问题\n",
    "        print('step3:使用query_engine回答问题')\n",
    "        rag_predictions_dataset=await rag_dataset.amake_predictions_with(\n",
    "            predictor=query_engine,\n",
    "            batch_size=10,\n",
    "            sleep_time_in_seconds=2\n",
    "            )\n",
    "        rag_predictions_dataset.save_json(ragdataset_predictions_path)\n",
    "    \n",
    "    return rag_dataset,rag_predictions_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义评估函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from llama_index.core.evaluation import BatchEvalRunner\n",
    "from llama_index.core.evaluation import (\n",
    "    AnswerRelevancyEvaluator,\n",
    "    ContextRelevancyEvaluator,\n",
    "    CorrectnessEvaluator,\n",
    "    FaithfulnessEvaluator,\n",
    ")\n",
    "\n",
    "runner=BatchEvalRunner(\n",
    "    evaluators={\n",
    "        \"answer_relevancy\":AnswerRelevancyEvaluator(),\n",
    "        \"context_relevancy\":ContextRelevancyEvaluator(),\n",
    "        \"correctness\":CorrectnessEvaluator(),\n",
    "        \"faithfulness\":FaithfulnessEvaluator()        \n",
    "    },\n",
    "    workers=12,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "async def eval_query_engine(queries:List[str],contexts_list:List[List[str]],response_strs:List[str]):\n",
    "    eval_results=await runner.aevaluate_response_strs(\n",
    "        queries=queries,\n",
    "        contexts_list=contexts_list,\n",
    "        response_strs=response_strs\n",
    "    )\n",
    "\n",
    "    for key in eval_results.keys():\n",
    "        results = eval_results[key]\n",
    "        scores = 0\n",
    "        for result in results:\n",
    "            score = getattr(result,'score',0)\n",
    "            if score:\n",
    "                scores += score\n",
    "        score = scores / len(results)\n",
    "        print(f\"{key} Score: {score}\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 所有文档构建1个查询引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 140/140 [00:00<00:00, 474.34file/s]\n",
      "c:\\Users\\wushaogui\\miniconda3\\envs\\langchian\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Parsing nodes: 100%|██████████| 140/140 [00:00<00:00, 144.45it/s]\n",
      "Generating embeddings: 100%|██████████| 963/963 [01:12<00:00, 13.33it/s]\n",
      "100%|██████████| 80/80 [03:21<00:00,  2.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy Score: 0.75\n",
      "context_relevancy Score: 0.7375\n",
      "correctness Score: 2.925\n",
      "faithfulness Score: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# 1.读取数据\n",
    "documents=SimpleDirectoryReader(input_dir='../../data/sidaminzhu',recursive=True).load_data(show_progress=True)\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "nodes = splitter.get_nodes_from_documents(documents,show_progress=True)\n",
    "\n",
    "# 2.所有文档生成1个查询引擎\n",
    "from llama_index.core import VectorStoreIndex\n",
    "index=VectorStoreIndex(nodes=nodes,show_progress=True)\n",
    "query_engine=index.as_query_engine()\n",
    "\n",
    "# 3.基于node及查询引擎生成测试数据\n",
    "rag_dataset,rag_predictions_dataset=await Build_test_dataset(\n",
    "    nodes,query_engine,test_size=20,prefix='OneEngine')\n",
    "\n",
    "# 4.测试\n",
    "queries=[example.query for example in rag_dataset.examples]\n",
    "contexts_list=[example.reference_contexts for example in rag_dataset.examples]\n",
    "response_strs=[example.response for example in rag_predictions_dataset.predictions]\n",
    "await eval_query_engine(queries,contexts_list,response_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每个文档构建1个查询引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 41/41 [00:00<00:00, 2641.98file/s]\n",
      "Loading files: 100%|██████████| 34/34 [00:00<00:00, 2833.31file/s]\n",
      "Loading files: 100%|██████████| 26/26 [00:00<00:00, 2888.79file/s]\n",
      "Loading files: 100%|██████████| 39/39 [00:00<00:00, 1559.21file/s]\n",
      "Generating embeddings: 100%|██████████| 127/127 [00:09<00:00, 12.74it/s]\n",
      "Generating embeddings: 100%|██████████| 312/312 [00:21<00:00, 14.41it/s]\n",
      "Generating embeddings: 100%|██████████| 340/340 [00:24<00:00, 13.91it/s]\n",
      "Generating embeddings: 100%|██████████| 184/184 [00:13<00:00, 13.36it/s]\n",
      "100%|██████████| 80/80 [03:43<00:00,  2.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy Score: 0.7\n",
      "context_relevancy Score: 0.866875\n",
      "correctness Score: 2.825\n",
      "faithfulness Score: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "# 1.分别读取四大名著\n",
    "dir_list=glob.glob('../../data/sidaminzhu/*')\n",
    "documents=[\n",
    "    SimpleDirectoryReader(input_dir).load_data(show_progress=True)\n",
    "    for input_dir in dir_list\n",
    " ]\n",
    "\n",
    "# 2.构建四大名著的nodes\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "documents_nodes=[\n",
    "    splitter.get_nodes_from_documents(document)\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "# 3.构建四大名著的indexs\n",
    "documents_indexs=[\n",
    "    VectorStoreIndex(nodes=nodes,show_progress=True)\n",
    "    for nodes in documents_nodes\n",
    "]\n",
    "\n",
    "# 4.构建四大名著的query_engine，并构建回复函数\n",
    "def get_doc_tools(\n",
    "    vector_index,name: str,\n",
    ") -> str:\n",
    "    \n",
    "    def vector_query(query: str) -> str:\n",
    "        f'''设计用于回答关于{name}的问题\n",
    "        query : 输入内容\n",
    "        '''\n",
    "\n",
    "        query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\", fn=vector_query,description=f\"关于{name}问题的回答助手\"\n",
    "    )\n",
    "\n",
    "    return vector_query_tool\n",
    "\n",
    "# 5.封装四大名著的query_engine为tools\n",
    "dir_info=[os.path.split(dir)[1].replace('白话文','') for dir in dir_list]\n",
    "documents_tools=[\n",
    "    get_doc_tools(vector_index,dir_info[i])\n",
    "    for i,vector_index in enumerate(documents_indexs)\n",
    "]\n",
    "\n",
    "# 6.将四大名著的tools封装为1个index，并生成检索器\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "tool_index=ObjectIndex.from_objects(\n",
    "    documents_tools,\n",
    "    index_cls=VectorStoreIndex\n",
    ")\n",
    "tool_retriever=tool_index.as_retriever(similarity_top_k=1)\n",
    "\n",
    "# 7.基于FunctionCallingAgent生成测试数据集\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tool_retriever=tool_retriever,\n",
    "    system_prompt=\"\"\"You are an agent designed to answer queries over a set of given documents.\n",
    "    Please use the tools provided to answer a question as possible. Do not rely on prior knowledge\\\n",
    "    \"\"\",\n",
    "    verbose=False,\n",
    ")\n",
    "rag_dataset,rag_predictions_dataset=await Build_test_dataset(\n",
    "    documents_nodes,agent,test_size=20,prefix='OneEngine')\n",
    "\n",
    "# 8.评估\n",
    "queries=[example.query for example in rag_dataset.examples]\n",
    "contexts_list=[example.reference_contexts for example in rag_dataset.examples]\n",
    "response_strs=[example.response for example in rag_predictions_dataset.predictions]\n",
    "await eval_query_engine(queries,contexts_list,response_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每个文档构建2个查询引擎"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DocumentSummaryIndex', 'EmptyIndex', 'GPTDocumentSummaryIndex', 'GPTEmptyIndex', 'GPTKeywordTableIndex', 'GPTListIndex', 'GPTPandasIndex', 'GPTRAKEKeywordTableIndex', 'GPTSQLStructStoreIndex', 'GPTSimpleKeywordTableIndex', 'GPTTreeIndex', 'GPTVectorStoreIndex', 'KeywordTableIndex', 'KnowledgeGraphIndex', 'ListIndex', 'MultiModalVectorStoreIndex', 'PandasIndex', 'PropertyGraphIndex', 'RAKEKeywordTableIndex', 'SQLStructStoreIndex', 'SimpleKeywordTableIndex', 'SummaryIndex', 'TreeIndex', 'VectorStoreIndex']\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import indices\n",
    "\n",
    "indexs=list(filter(lambda att:att.find('Index')>0,dir(indices)))\n",
    "print(indexs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由以上输出可以，llamaindex索引内容的方式有多种，以下选择KeywordTableIndex,VectorStoreIndex分别索引1个文档，检索时，llm根据问题选择不同方式检索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 41/41 [00:00<00:00, 2733.10file/s]\n",
      "Loading files: 100%|██████████| 34/34 [00:00<00:00, 2425.03file/s]\n",
      "Loading files: 100%|██████████| 26/26 [00:00<00:00, 2129.46file/s]\n",
      "Loading files: 100%|██████████| 39/39 [00:00<00:00, 2166.39file/s]\n",
      "Generating embeddings: 100%|██████████| 127/127 [00:10<00:00, 12.55it/s]\n",
      "Generating embeddings: 100%|██████████| 312/312 [00:23<00:00, 13.55it/s]\n",
      "Generating embeddings: 100%|██████████| 340/340 [00:23<00:00, 14.34it/s]\n",
      "Generating embeddings: 100%|██████████| 184/184 [00:12<00:00, 14.22it/s]\n",
      "Extracting keywords from nodes: 100%|██████████| 127/127 [03:51<00:00,  1.83s/it]\n",
      "Extracting keywords from nodes: 100%|██████████| 312/312 [12:08<00:00,  2.33s/it]\n",
      "Extracting keywords from nodes: 100%|██████████| 340/340 [10:55<00:00,  1.93s/it]\n",
      "Extracting keywords from nodes: 100%|██████████| 184/184 [05:16<00:00,  1.72s/it]\n",
      "100%|██████████| 80/80 [02:33<00:00,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy Score: 0.775\n",
      "context_relevancy Score: 0.76375\n",
      "correctness Score: 2.9\n",
      "faithfulness Score: 0.15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "from llama_index.core import KeywordTableIndex,VectorStoreIndex\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# 1.分别读取生成四大名著的nodes、indexs\n",
    "dir_list=glob.glob('../../data/sidaminzhu/*')\n",
    "documents=[\n",
    "    SimpleDirectoryReader(input_dir).load_data(show_progress=True)\n",
    "    for input_dir in dir_list\n",
    " ]\n",
    "\n",
    "splitter = SentenceSplitter(chunk_size=1024)\n",
    "documents_nodes=[\n",
    "    splitter.get_nodes_from_documents(document)\n",
    "    for document in documents\n",
    "]\n",
    "\n",
    "vector_indexs=[\n",
    "    VectorStoreIndex(nodes=nodes,show_progress=True)\n",
    "    for nodes in documents_nodes\n",
    "]\n",
    "\n",
    "keyword_indexs=[\n",
    "    KeywordTableIndex(nodes=nodes,show_progress=True)\n",
    "    for nodes in documents_nodes\n",
    "]\n",
    "\n",
    "# 2.将不同索引方式封装在一起\n",
    "def get_doc_tools(\n",
    "    vector_index,summary_indexs,name: str,\n",
    ") -> str:\n",
    "    \n",
    "    def vector_query(query: str) -> str:\n",
    "        f'''通过语义相关查询回答关于{name}的问题，擅长精确查询答案\n",
    "        query : 输入内容\n",
    "        '''\n",
    "\n",
    "        query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\", fn=vector_query,description=f\"关于{name}问题的回答助手\"\n",
    "    )\n",
    "\n",
    "    def keyword_query(query: str) -> str:\n",
    "        f'''回答关于{name}的问题，擅长输出归纳性总结\n",
    "        query : 输入内容\n",
    "        '''\n",
    "\n",
    "        query_engine = keyword_indexs.as_query_engine(\n",
    "                response_mode=\"tree_summarize\",\n",
    "                use_async=True,\n",
    "        )\n",
    "        response = query_engine.query(query)\n",
    "        return response\n",
    "    \n",
    "    vector_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"vector_tool_{name}\", fn=vector_query,description=f\"关于{name}问题的回答助手\"\n",
    "    )\n",
    "\n",
    "    keyword_query_tool = FunctionTool.from_defaults(\n",
    "        name=f\"keyword_tool_{name}\", fn=keyword_query,description=f\"关于{name}问题的回答助手\"\n",
    "    )\n",
    "\n",
    "    return vector_query_tool,keyword_query_tool\n",
    "\n",
    "dir_info=[os.path.split(dir)[1].replace('白话文','') for dir in dir_list]\n",
    "documents_tools=[\n",
    "    get_doc_tools(vector_index,keyword_index,dir_info[i])\n",
    "    for i,(vector_index,keyword_index) in enumerate(zip(vector_indexs,keyword_indexs))\n",
    "]\n",
    "all_tools = [t for documents_tools in documents_tools for t in documents_tools] # 注意：这里把所有文档的2个索引工具都放在一起\n",
    "\n",
    "# 3.将所有检索工具封装到一起\n",
    "from llama_index.core.objects import ObjectIndex\n",
    "tool_index=ObjectIndex.from_objects(\n",
    "    all_tools,\n",
    "    index_cls=VectorStoreIndex\n",
    ")\n",
    "tool_retriever=tool_index.as_retriever(similarity_top_k=2)\n",
    "\n",
    "# 4.生成测试数据\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tool_retriever=tool_retriever,\n",
    "    system_prompt=\"\"\"You are an agent designed to answer queries over a set of given documents.\n",
    "    Please use the tools provided to answer a question as possible. Do not rely on prior knowledge\\\n",
    "    \"\"\"\n",
    ")\n",
    "rag_dataset,rag_predictions_dataset=await Build_test_dataset(\n",
    "    documents_nodes,agent,test_size=20,prefix='OneEngine')\n",
    "\n",
    "# 5. 评估\n",
    "queries=[example.query for example in rag_dataset.examples]\n",
    "contexts_list=[example.reference_contexts for example in rag_dataset.examples]\n",
    "response_strs=[example.response for example in rag_predictions_dataset.predictions]\n",
    "\n",
    "await eval_query_engine(queries,contexts_list,response_strs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 每个文档构建2个查询引擎2\n",
    "\n",
    "前面生成agent时，所有的engine都一起放入agent，可能存在问题，以下先通过RouterQueryEngine汇总一个文档的所有engine，再放到agent中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 80/80 [02:44<00:00,  2.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer_relevancy Score: 0.775\n",
      "context_relevancy Score: 0.745\n",
      "correctness Score: 2.925\n",
      "faithfulness Score: 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import glob \n",
    "from llama_index.core import KeywordTableIndex,VectorStoreIndex\n",
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "\n",
    "# 1.分别读取生成四大名著的nodes、indexs\n",
    "# dir_list=glob.glob('../../data/sidaminzhu/*')\n",
    "# documents=[\n",
    "#     SimpleDirectoryReader(input_dir).load_data(show_progress=True)\n",
    "#     for input_dir in dir_list\n",
    "#  ]\n",
    "\n",
    "# splitter = SentenceSplitter(chunk_size=1024)\n",
    "# documents_nodes=[\n",
    "#     splitter.get_nodes_from_documents(document)\n",
    "#     for document in documents\n",
    "# ]\n",
    "\n",
    "# vector_indexs=[\n",
    "#     VectorStoreIndex(nodes=nodes,show_progress=True)\n",
    "#     for nodes in documents_nodes\n",
    "# ]\n",
    "\n",
    "# keyword_indexs=[\n",
    "#     KeywordTableIndex(nodes=nodes,show_progress=True)\n",
    "#     for nodes in documents_nodes\n",
    "# ]\n",
    "\n",
    "# 2. 通过QueryEngineTool将每个文档的KeywordTableIndex与VectorStoreIndex索引封装为一个查询引擎\n",
    "from llama_index.core.query_engine.router_query_engine import RouterQueryEngine\n",
    "from llama_index.core.selectors import LLMSingleSelector\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "\n",
    "documents_engines=[]\n",
    "for i,(vector_index,keyword_index) in enumerate(zip(vector_indexs,keyword_indexs)):\n",
    "    vector_query_engine = vector_index.as_query_engine(similarity_top_k=2)\n",
    "    vector_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=vector_query_engine,\n",
    "        description=(\n",
    "            \"Useful for retrieving specific context from the documents\"\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    keyword_query_engine = keyword_index.as_query_engine(response_mode=\"tree_summarize\",use_async=True)\n",
    "    summary_tool = QueryEngineTool.from_defaults(\n",
    "        query_engine=keyword_query_engine,\n",
    "        description=(\"Useful for summarization questions related to documents\"),\n",
    "    )\n",
    "\n",
    "    documents_engines.append(\n",
    "        RouterQueryEngine(\n",
    "            selector=LLMSingleSelector.from_defaults(),\n",
    "            query_engine_tools=[vector_tool,summary_tool],\n",
    "            verbose=True)\n",
    "        )\n",
    "\n",
    "# 3. 通过QueryEngineTool将所有文档的查询引擎融合到一起\n",
    "from llama_index.core.tools import QueryEngineTool,ToolMetadata\n",
    "dir_info=[os.path.split(dir)[1].replace('白话文','') for dir in dir_list]\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=engine,\n",
    "        metadata=ToolMetadata(name=f\"query_engine_{dir_info[i]}\",description=f\"回答关于{dir_info[i]}的问题\")\n",
    "        )   \n",
    "for i,engine in enumerate(documents_engines)]\n",
    "\n",
    "# 4. 生成测试数据\n",
    "from llama_index.core.agent import FunctionCallingAgent\n",
    "agent = FunctionCallingAgent.from_tools(\n",
    "    tools=query_engine_tools,\n",
    "    system_prompt=\"\"\"You are an agent designed to answer queries over a set of given documents.\n",
    "    Please use the tools provided to answer a question as possible. Do not rely on prior knowledge\\\n",
    "    \"\"\",\n",
    "    verbose=True,\n",
    ")\n",
    "rag_dataset,rag_predictions_dataset=await Build_test_dataset(\n",
    "    documents_nodes,agent,test_size=20,prefix='OneEngine')\n",
    "\n",
    "# 5.评估\n",
    "queries=[example.query for example in rag_dataset.examples]\n",
    "contexts_list=[example.reference_contexts for example in rag_dataset.examples]\n",
    "response_strs=[example.response for example in rag_predictions_dataset.predictions]\n",
    "await eval_query_engine(queries,contexts_list,response_strs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchian",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
